{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usual imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# torch and torchvision in order to build the neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as Func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv_1_64): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_64_64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_64_128): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_128_128): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=6272, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc5): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (batch_norm_64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_128_2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_128_1d): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_256): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_512): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_1024): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define neural network architecture\n",
    "# based on VGG style neural network\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        \n",
    "        # N x N x 1 --> N x N x 64\n",
    "        self.conv_1_64 = nn.Conv2d(1, 64, 3, padding=1)\n",
    "        \n",
    "        # N x N x 64 --> N x N x 64\n",
    "        self.conv_64_64 = nn.Conv2d(64,64,3,padding = 1) \n",
    "        \n",
    "        # N x N x 64 --> N x N x 128\n",
    "        self.conv_64_128 = nn.Conv2d(64, 128, 3, padding = 1) \n",
    "        \n",
    "        # N x N x 128 --> N x N x 128\n",
    "        self.conv_128_128 = nn.Conv2d(128,128,3,padding = 1)\n",
    "        \n",
    "        # N in case of MNIST is 7, so we go from 7 x 7 x 128 to a 1024, with further layers of 512, 256, 128 and finally 10\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 10)\n",
    "        \n",
    "        # initiate dropout layer with a given probability\n",
    "        self.dropout = nn.Dropout(p=0.5);\n",
    "        \n",
    "        # initiate batch normalizations\n",
    "        self.batch_norm_64 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm_128_2d = nn.BatchNorm2d(128)\n",
    "        self.batch_norm_128_1d = nn.BatchNorm1d(128)\n",
    "        self.batch_norm_256 = nn.BatchNorm1d(256)\n",
    "        self.batch_norm_512 = nn.BatchNorm1d(512)\n",
    "        self.batch_norm_1024 = nn.BatchNorm1d(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input --> CNN 1, final dimensions 28 x 28 x 64\n",
    "        x = F.relu(self.conv_1_64(x))\n",
    "        x = self.batch_norm_64(x)\n",
    "        \n",
    "        # CNN 1 --> CNN 2 --> CNN 3, final dimensions 28 x 28 x 64\n",
    "        x = self.batch_norm_64(F.relu(self.conv_64_64(x)))\n",
    "        x = self.batch_norm_64(F.relu(self.conv_64_64(x)))\n",
    "        \n",
    "        # max pool, final dimensions 14 x 14 x 64\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        \n",
    "        # CNN 3 --> CNN 4 --> CNN 5 --> CNN 6, final dimensions 14 x 14 x 128\n",
    "        x = self.batch_norm_128_2d(F.relu(self.conv_64_128(x)))\n",
    "        x = self.batch_norm_128_2d(F.relu(self.conv_128_128(x)))\n",
    "        x = self.batch_norm_128_2d(F.relu(self.conv_128_128(x)))\n",
    "        \n",
    "        # max pool, final dimensions 7 x 7 x 128\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        \n",
    "        # convert to a vector\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        # fully connected layers\n",
    "        x = self.dropout(self.batch_norm_1024(F.relu(self.fc1(x))))\n",
    "        x = self.dropout(self.batch_norm_512(F.relu(self.fc2(x))))\n",
    "        x = self.dropout(self.batch_norm_256(F.relu(self.fc3(x))))\n",
    "        x = self.dropout(self.batch_norm_128_1d(F.relu(self.fc4(x))))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "netCNN = Net()\n",
    "\n",
    "# select device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(netCNN.parameters(), lr=0.001)\n",
    "netCNN.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_networks = 2\n",
    "batchsize = 500\n",
    "num_epochs = 5\n",
    "\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "# for convolutional neural network\n",
    "x_train = pd.read_csv('train.csv')\n",
    "x_test = pd.read_csv('test.csv')\n",
    "y_train = x_train['label'].values\n",
    "x_train = x_train.drop(['label'],1).values\n",
    "x_test = x_test.values\n",
    "\n",
    "x_train = x_train.reshape(np.shape(x_train)[0],1,img_rows,img_cols)\n",
    "x_test = x_test.reshape(np.shape(x_test)[0],1,img_rows,img_cols)\n",
    "\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "\n",
    "train = torch.from_numpy(x_train)\n",
    "test = torch.from_numpy(x_test)\n",
    "train_label = torch.from_numpy(y_train)\n",
    "train_label = train_label.long()\n",
    "\n",
    "num_iterations = int(train.size(0)/batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 180.953\n",
      "Epoch 2 loss: 31.413\n",
      "Finished Training\n",
      "0.9790952380952381\n",
      "Epoch 1 loss: 164.631\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fe7a15dc8ba8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# it seems that these have to be done one at a time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mtrain_pil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mtrain_temp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maffine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Python37\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \"\"\"\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Python37\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[1;34m(pic, mode)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mnpimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mpic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mnpimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "affine = transforms.RandomAffine(degrees=10, translate=(.20,.20), scale=(.9,1.1))\n",
    "output_all = torch.empty(test.size(0),10,num_networks)\n",
    "output_all.to(device)\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "for n_network in range(num_networks):\n",
    "    train_temp = torch.empty(np.shape(x_train)[0],1,img_rows,img_cols)\n",
    "    netCNN = Net()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(netCNN.parameters(), lr=0.001)\n",
    "    netCNN.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # it seems that these have to be done one at a time\n",
    "        for n_input, input_data in enumerate(train):\n",
    "            train_pil = transforms.ToPILImage(mode=None)(train[n_input])\n",
    "            train_temp[n_input,:,:,:] = Func.to_tensor(affine(train_pil))\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i in range(num_iterations):\n",
    "\n",
    "            inputs = train_temp[i*batchsize:(i+1)*batchsize]\n",
    "            labels = train_label[i*batchsize:(i+1)*batchsize]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = netCNN(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch %d loss: %.3f' % (epoch + 1, running_loss/batchsize*1000))\n",
    "        running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    prediction_all = torch.empty(train.size(0),1)\n",
    "    for i in range(num_iterations):\n",
    "        test_input = train[i*batchsize:(i+1)*batchsize]\n",
    "        output = netCNN(test_input.to(device))\n",
    "        _, prediction = torch.max(output,1)\n",
    "        prediction_all[i*batchsize:(i+1)*batchsize,0] = prediction\n",
    "\n",
    "    match = train_label.int().reshape(train.size(0),1) == prediction_all.cpu().int()\n",
    "    fraction = int(match.sum())/int(prediction_all.shape[0])\n",
    "    print(fraction)\n",
    "    \n",
    "    # prediction of the current net\n",
    "    num_iterations = int(test.size(0)/batchsize)\n",
    "    prediction_all = torch.empty(test.size(0),1)\n",
    "    for i in range(num_iterations):\n",
    "        test_input = test[i*batchsize:(i+1)*batchsize]\n",
    "        output_all[i*batchsize:(i+1)*batchsize,:,n_network] = netCNN(test_input.to(device))\n",
    "        _, prediction = torch.max(output,1)\n",
    "        prediction_all[i*batchsize:(i+1)*batchsize,0] = prediction\n",
    "    predicted_cpu = prediction_all.cpu()\n",
    "    predicted_final = np.array(predicted_cpu)\n",
    "    predicted_final = predicted_final.astype(int)\n",
    "    \n",
    "\n",
    "runtime = time.time() - time_start\n",
    "print('Elapsed Time: %.3f seconds' % runtime)\n",
    "\n",
    "image_id = np.linspace(1,test.size(0),test.size(0))\n",
    "image_id.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # prediction of the current net\n",
    "num_iterations = int(test.size(0)/batchsize)\n",
    "prediction_all = torch.empty(test.size(0),1)\n",
    "for i in range(num_iterations):\n",
    "    test_input = test[i*batchsize:(i+1)*batchsize]\n",
    "    output = netCNN(test_input.to(device))\n",
    "    _, prediction = torch.max(output,1)\n",
    "    prediction_all[i*batchsize:(i+1)*batchsize,0] = prediction\n",
    "predicted_cpu = prediction_all.cpu()\n",
    "predicted_final = np.array(predicted_cpu)\n",
    "predicted_final = predicted_final.astype(int)\n",
    "\n",
    "\n",
    "output_file = 'submission.csv'\n",
    "#print(predicted_final)\n",
    "\n",
    "with open(output_file, 'w') as f :\n",
    "    f.write('ImageId,Label\\n')\n",
    "    for i in range(len(predicted_final)) :\n",
    "        f.write(\"\".join([str(i+1),',',str(predicted_final[i][0]),'\\n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24f9337ce80>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test out the pytorch augmentations\n",
    "\n",
    "index_ = 46;\n",
    "\n",
    "#rotation = transforms.RandomRotation(degrees=30)\n",
    "crop = transforms.CenterCrop(20)\n",
    "affine = transforms.RandomAffine(degrees=0, scale=(2,2))\n",
    "\n",
    "train_pil = transforms.ToPILImage(mode=None)(train[index_])\n",
    "#train_pil = rotation(train_pil)\n",
    "train_pil = affine(train_pil)\n",
    "train_temp = Func.to_tensor(train_pil)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(train_temp[-1])\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(train[index_][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
